{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y'all punks can get the data from kaggle here: https://www.kaggle.com/uciml/sms-spam-collection-dataset/kernels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/spam.csv', encoding = 'latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  classification                                            message\n",
       "0            ham  Go until jurong point, crazy.. Available only ...\n",
       "1            ham                      Ok lar... Joking wif u oni...\n",
       "2           spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3            ham  U dun say so early hor... U c already then say...\n",
       "4            ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from html import parser\n",
    "\n",
    "# Drop the empty columns\n",
    "df = df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "\n",
    "# Rename the columns\n",
    "df = df.rename(index=str, columns={\"v1\": \"classification\", \"v2\": \"message\"})\n",
    "\n",
    "# Remove HTML encodings\n",
    "df['message'] = df['message'].apply(parser.unescape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['classification'] = pd.get_dummies(df['classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   classification                                            message\n",
      "0               1  Go until jurong point, crazy.. Available only ...\n",
      "1               1                      Ok lar... Joking wif u oni...\n",
      "2               0  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3               1  U dun say so early hor... U c already then say...\n",
      "4               1  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(df['classification'].values.astype(np.int))\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "\n",
    "y = np.array(df['classification'].values)\n",
    "X = df['message'].values\n",
    "#print(X)\n",
    "\n",
    "#y = [0 if y_ == \"spam\" else 1 for y_ in y]\n",
    "y_ohe = keras.utils.to_categorical(y)\n",
    "y_ohe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find how long our longest message is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "910\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "max = 0\n",
    "count = 0\n",
    "for i in X:\n",
    "    if len(i) > max:\n",
    "        #if len(i) < 200:\n",
    "        max = len(i)\n",
    "        #else:\n",
    "            #count +=1\n",
    "        \n",
    "print(max)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "NUM_TOP_WORDS = None\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
    "tokenizer.fit_on_texts(X)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "sequences = pad_sequences(sequences)\n",
    "MAX_TEXT_LEN = len(sequences[0])\n",
    "print(MAX_TEXT_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = y.astype(bool)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sequences, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "\n",
    "# truncate and pad input sequences to be the same length\n",
    "max_text_length = 189\n",
    "\n",
    "NUM_CLASSES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "(8920, 100)\n"
     ]
    }
   ],
   "source": [
    "EMBED_SIZE = 100\n",
    "# the embed size should match the file you load glove from\n",
    "embeddings_index = {}\n",
    "f = open('data/glove.6B/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "# save key/array pairs of the embeddings\n",
    "#  the key of the dictionary is the word, the array is the embedding\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# now fill in the matrix, using the ordering from the\n",
    "#  keras word tokenizer from before\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        (None, 189)               0         \n",
      "_________________________________________________________________\n",
      "embedding_22 (Embedding)     (None, 189, 100)          892000    \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 100)               60300     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 952,401\n",
      "Trainable params: 60,401\n",
      "Non-trainable params: 892,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Input, Model\n",
    "from keras.layers import Dense, GRU, Activation\n",
    "from keras.regularizers import l2\n",
    "\n",
    "l2_lambda = 0.0001\n",
    "\n",
    "input_holder = Input(shape=(189,))\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_TEXT_LEN,\n",
    "                            trainable=False)(input_holder)\n",
    "\n",
    "gru_layer = GRU(100,dropout=0.2, recurrent_dropout=0.2)(embedding_layer)\n",
    "dense1 = Dense(NUM_CLASSES, \n",
    "                activation='sigmoid',\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                kernel_regularizer=l2(l2_lambda)\n",
    "             )(gru_layer)\n",
    "\n",
    "rnn_gru = Model(inputs=input_holder, outputs=dense1)\n",
    "rnn_gru.compile(loss='binary_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "print(rnn_gru.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4457 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4457/4457 [==============================] - 18s - loss: 0.2159 - acc: 0.9096 - val_loss: 0.1078 - val_acc: 0.9623\n",
      "Epoch 2/3\n",
      "4457/4457 [==============================] - 16s - loss: 0.1011 - acc: 0.9650 - val_loss: 0.0882 - val_acc: 0.9722\n",
      "Epoch 3/3\n",
      "4457/4457 [==============================] - 16s - loss: 0.0842 - acc: 0.9731 - val_loss: 0.0798 - val_acc: 0.9758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2931a913c18>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_gru.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115/1115 [==============================] - 2s     \n"
     ]
    }
   ],
   "source": [
    "yhat_gru = np.round(rnn_gru.predict(X_test, verbose=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.996894409938\n",
      "0.975784753363\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics as mt\n",
    "rec = mt.recall_score(y_test,yhat_gru)\n",
    "acc = mt.accuracy_score(y_test,yhat_gru)\n",
    "print(rec)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        (None, 189)               0         \n",
      "_________________________________________________________________\n",
      "embedding_21 (Embedding)     (None, 189, 100)          892000    \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 972,501\n",
      "Trainable params: 80,501\n",
      "Non-trainable params: 892,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Input, Model\n",
    "from keras.layers import Dense, LSTM, Activation\n",
    "from keras.regularizers import l2\n",
    "\n",
    "l2_lambda = 0.0001\n",
    "\n",
    "input_holder = Input(shape=(189,))\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_TEXT_LEN,\n",
    "                            trainable=False)(input_holder)\n",
    "\n",
    "lstm_layer = LSTM(100,dropout=0.2, recurrent_dropout=0.2)(embedding_layer)\n",
    "dense1 = Dense(NUM_CLASSES, \n",
    "                activation='sigmoid',\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                kernel_regularizer=l2(l2_lambda)\n",
    "             )(lstm_layer)\n",
    "\n",
    "rnn_lstm = Model(inputs=input_holder, outputs=dense1)\n",
    "rnn_lstm.compile(loss='binary_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "print(rnn_lstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4457 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4457/4457 [==============================] - 22s - loss: 0.1926 - acc: 0.9242 - val_loss: 0.1238 - val_acc: 0.9614\n",
      "Epoch 2/3\n",
      "4457/4457 [==============================] - 20s - loss: 0.1071 - acc: 0.9628 - val_loss: 0.0883 - val_acc: 0.9740\n",
      "Epoch 3/3\n",
      "4457/4457 [==============================] - 20s - loss: 0.0908 - acc: 0.9686 - val_loss: 0.0833 - val_acc: 0.9722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29316a3b780>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_lstm.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115/1115 [==============================] - 3s     \n"
     ]
    }
   ],
   "source": [
    "yhat = np.round(rnn_lstm.predict(X_test, verbose=1))\n",
    "#yhat = np.array([[0,1] if np.argmax(x) == 1 else [1,0] for x in yhat]).astype(float)\n",
    "#yhat = [list(x).index(1.0) for x in list(yhat)]\n",
    "\n",
    "#y_test = np.array([[0,1] if np.argmax(x) == 1 else [1,0] for x in y_test]).astype(float)\n",
    "#y_test = [list(x).index(1.0) for x in list(y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.993788819876\n",
      "0.972197309417\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics as mt\n",
    "rec = mt.recall_score(y_test,yhat)\n",
    "acc = mt.accuracy_score(y_test,yhat)\n",
    "print(rec)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ..., 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99228644]\n",
      " [ 0.99404907]\n",
      " [ 0.990049  ]\n",
      " ..., \n",
      " [ 0.99001336]\n",
      " [ 0.99323457]\n",
      " [ 0.99784422]]\n"
     ]
    }
   ],
   "source": [
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float32"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(yhat[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.bool_"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gs_make_lstm(l2_lambda=0.0001, dropout=0.2, recurrent_dropout=0.2, activation='sigmoid', kernel_initializer='glorot_uniform'):\n",
    "    input_holder = Input(shape=(189,))\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBED_SIZE,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_TEXT_LEN,\n",
    "                                trainable=False)(input_holder)\n",
    "\n",
    "    lstm_layer = LSTM(100,dropout=dropout, recurrent_dropout=recurrent_dropout)(embedding_layer)\n",
    "    dense1 = Dense(NUM_CLASSES, \n",
    "                    activation=activation,\n",
    "                    kernel_initializer=kernel_initializer,\n",
    "                    kernel_regularizer=l2(l2_lambda)\n",
    "                 )(lstm_layer)\n",
    "\n",
    "    rnn_lstm_gs = Model(inputs=input_holder, outputs=dense1)\n",
    "    rnn_lstm_gs.compile(loss='binary_crossentropy', \n",
    "                  optimizer='rmsprop', \n",
    "                  metrics=['accuracy'])\n",
    "    return rnn_lstm_gs\n",
    "\n",
    "def gs_make_gru(l2_lambda=0.0001, dropout=0.2, recurrent_dropout=0.2, activation='sigmoid', kernel_initializer='glorot_uniform'):\n",
    "    input_holder = Input(shape=(189,))\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBED_SIZE,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_TEXT_LEN,\n",
    "                                trainable=False)(input_holder)\n",
    "\n",
    "    gru_layer = GRU(100,dropout=dropout, recurrent_dropout=recurrent_dropout)(embedding_layer)\n",
    "    dense1 = Dense(NUM_CLASSES, \n",
    "                    activation=activation,\n",
    "                    kernel_initializer=kernel_initializer,\n",
    "                    kernel_regularizer=l2(l2_lambda)\n",
    "                 )(gru_layer)\n",
    "\n",
    "    rnn_gru = Model(inputs=input_holder, outputs=dense1)\n",
    "    rnn_gru.compile(loss='binary_crossentropy', \n",
    "                  optimizer='rmsprop', \n",
    "                  metrics=['accuracy'])\n",
    "    print(rnn_gru.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM\n",
    "model = KerasClassifier(build_fn=gs_make_lstm)\n",
    "param_grid = {'epochs'=[2,4,6], 'l2_lambda'=[0.000001,0.0001,0.01], 'dropout'=[0.1,0.2,0.3], 'recurrent_dropout'=[0.1,0.2,0.3], 'activation'=['sigmoid', 'relu'], 'kernel_initializer'=['glorot_uniform','he_normal']}\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRU\n",
    "model = KerasClassifier(build_fn=gs_make_gru)\n",
    "param_grid = {'epochs'=[2,4,6], 'l2_lambda'=[0.000001,0.0001,0.01], 'dropout'=[0.1,0.2,0.3], 'recurrent_dropout'=[0.1,0.2,0.3], 'activation'=['sigmoid', 'relu'], 'kernel_initializer'=['glorot_uniform','he_normal']}\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
